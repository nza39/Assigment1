{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db06590b",
   "metadata": {},
   "source": [
    "# Assigment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9ee0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'Emails.txt', 'Recipes.txt', 'newsArticle.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.util import bigrams\n",
    "\n",
    "corpus_root='./Corpus'\n",
    "\n",
    "\n",
    "myCorpus=PlaintextCorpusReader(corpus_root, '.*', encoding = \"utf8\")\n",
    "myCorpus.fileids()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc65ced",
   "metadata": {},
   "source": [
    "## Subcorpus 1 - Emails\n",
    "\n",
    "The file \"Email.txt\" is the first component of our corpus. This file contains emails sent by Natalia Zuluaga (natazulu03@gmail.com) and Ana Mayorca (anamayorcaalzate@gmail.com) in the spam of 3 years (2020-2023). Both participants concented to the collection of their information.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03705d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_words = myCorpus.words('Emails.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4a74a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8657"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length\n",
    "len(email_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfe5e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1716529975742174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lexical Diversity\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "lexical_diversity(email_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "935ce3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'a', 'peer', 'educator', ',', 'I', 'believe', 'that', 'I', 'will', 'be', 'able', 'to', 'motivate', 'students', 'by', 'reminding', 'them', 'that', 'is', 'completely', 'worth', 'it', 'to', 'make', 'an', 'effort', 'to', 'accomplish', 'things', 'academically', ',', 'not', 'just', 'to', 'fulfill', 'a', 'requirement', 'or', 'get', 'a', 'certain', 'grade', ',', 'but', 'to', 'feel', 'good', 'with', 'themselves', ',', 'grow', ',', 'learn', 'and', 'truly', 'take', 'advantage', 'of', 'this', 'massive', 'opportunity', 'we', 'have', 'been', 'given', '.']\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "#Longest sentence\n",
    "email_sentences = myCorpus.sents('Emails.txt')\n",
    "\n",
    "\n",
    "longestSentE=[]\n",
    "for i in email_sentences:\n",
    "    if len(i)>len(longestSentE):\n",
    "        longestSentE=i\n",
    "\n",
    "print(longestSentE)\n",
    "\n",
    "#Number of elements in the sentence\n",
    "print(len(longestSentE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e745758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'a', 'peer', 'educator', 'I', 'believe', 'that', 'I', 'will', 'be', 'able', 'to', 'motivate', 'students', 'by', 'reminding', 'them', 'that', 'is', 'completely', 'worth', 'it', 'to', 'make', 'an', 'effort', 'to', 'accomplish', 'things', 'academically', 'not', 'just', 'to', 'fulfill', 'a', 'requirement', 'or', 'get', 'a', 'certain', 'grade', 'but', 'to', 'feel', 'good', 'with', 'themselves', 'grow', 'learn', 'and', 'truly', 'take', 'advantage', 'of', 'this', 'massive', 'opportunity', 'we', 'have', 'been', 'given']\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "#Number of words in the sentence\n",
    "wordsSentE=[]\n",
    "\n",
    "for i in longestSentE:\n",
    "    if i.isalnum():\n",
    "        wordsSent.append(i)\n",
    "        \n",
    "print(wordsSentE)\n",
    "\n",
    "print(len(wordsSentE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea4c010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia Zuluaga; Ana Mayorca; --- Hello; Best regards; iPhone ---;\n",
      "Good evening; application process; --- Dear; Fall 2021; look forward;\n",
      "ZULND2103 ---; late January; still available; submit ACT; would like;\n",
      "--- Good; application fee; Zuluaga ---; start studying; test scores\n"
     ]
    }
   ],
   "source": [
    "#Top Collocations\n",
    "email = nltk.Text(myCorpus.words('Emails.txt'))\n",
    "email.collocations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96fe6d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 156),\n",
       " ('a', 83),\n",
       " ('are', 40),\n",
       " ('am', 31),\n",
       " ('application', 31),\n",
       " ('advance', 28),\n",
       " ('as', 27),\n",
       " ('at', 27),\n",
       " ('an', 25),\n",
       " ('again', 19)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten words that start with each of the vowels\n",
    "alistE=sorted((w.lower() for w in email_words if w.startswith('a')))\n",
    "\n",
    "vowelDist=FreqDist(alistE)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b74cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('email', 22),\n",
       " ('early', 8),\n",
       " ('evening', 8),\n",
       " ('everything', 7),\n",
       " ('each', 4),\n",
       " ('else', 4),\n",
       " ('experience', 4),\n",
       " ('enjoy', 3),\n",
       " ('enroll', 3),\n",
       " ('enrolled', 3)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist2E=sorted((w.lower() for w in email_words if w.startswith('e')))\n",
    "\n",
    "vowelDist=FreqDist(alist2E)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63d49dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 122),\n",
       " ('is', 96),\n",
       " ('it', 64),\n",
       " ('if', 53),\n",
       " ('iphone', 18),\n",
       " ('interested', 16),\n",
       " ('information', 11),\n",
       " ('inconvenience', 9),\n",
       " ('insect', 3),\n",
       " ('instead', 3)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist3E=sorted((w.lower() for w in email_words if w.startswith('i')))\n",
    "\n",
    "vowelDist=FreqDist(alist3E)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31c2689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 77),\n",
       " ('on', 44),\n",
       " ('or', 19),\n",
       " ('out', 12),\n",
       " ('other', 11),\n",
       " ('our', 9),\n",
       " ('opportunity', 7),\n",
       " ('one', 5),\n",
       " ('only', 5),\n",
       " ('offer', 4)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist4E=sorted((w.lower() for w in email_words if w.startswith('o')))\n",
    "\n",
    "vowelDist=FreqDist(alist4E)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54a814da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('us', 5),\n",
       " ('useful', 4),\n",
       " ('used', 3),\n",
       " ('unable', 2),\n",
       " ('unenroll', 2),\n",
       " ('up', 2),\n",
       " ('use', 2),\n",
       " ('using', 2),\n",
       " ('undergraduate', 1),\n",
       " ('understanding', 1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist5E=sorted((w.lower() for w in email_words if w.startswith('u')))\n",
    "\n",
    "vowelDist=FreqDist(alist5E)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d206ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'a', 'peer', 'educ', ',', 'i', 'believ', 'that', 'i', 'wil', 'be', 'abl', 'to', 'mot', 'stud', 'by', 'remind', 'them', 'that', 'is', 'complet', 'wor', 'it', 'to', 'mak', 'an', 'effort', 'to', 'accompl', 'thing', 'academ', ',', 'not', 'just', 'to', 'fulfil', 'a', 'requir', 'or', 'get', 'a', 'certain', 'grad', ',', 'but', 'to', 'feel', 'good', 'with', 'themselv', ',', 'grow', ',', 'learn', 'and', 'tru', 'tak', 'adv', 'of', 'thi', 'mass', 'opportun', 'we', 'hav', 'been', 'giv', '.']\n",
      "['as', 'a', 'peer', 'educ', ',', 'i', 'believ', 'that', 'i', 'will', 'be', 'abl', 'to', 'motiv', 'student', 'by', 'remind', 'them', 'that', 'is', 'complet', 'worth', 'it', 'to', 'make', 'an', 'effort', 'to', 'accomplish', 'thing', 'academ', ',', 'not', 'just', 'to', 'fulfil', 'a', 'requir', 'or', 'get', 'a', 'certain', 'grade', ',', 'but', 'to', 'feel', 'good', 'with', 'themselv', ',', 'grow', ',', 'learn', 'and', 'truli', 'take', 'advantag', 'of', 'thi', 'massiv', 'opportun', 'we', 'have', 'been', 'given', '.']\n"
     ]
    }
   ],
   "source": [
    "#A stemmed version of the longest sentence \n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "Stem1=[lancaster.stem(t) for t in longestSentE]\n",
    "\n",
    "Stem2=[porter.stem(t) for t in longestSentE]\n",
    "\n",
    "print(Stem1)\n",
    "print(Stem2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f024d5",
   "metadata": {},
   "source": [
    "## Subcorpus 2 - News Article\n",
    "The file \"newsArticle.txt\" is the second component of our corpus. This file is a compilation of news articles about Chat GPT. This articles were released by CNBC (https://www.cnbc.com/2023/02/07/microsoft-open-ai-chatgpt-event-2023-live-updates.html), The Helsinki Times (https://www.helsinkitimes.fi/themes/themes/science-and-technology/22938-an-interview-with-chat-gpt.html) and Tech Funding News (https://techfundingnews.com/what-is-chat-gpt-why-is-it-here-to-stay/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d245391",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_article = myCorpus.words('newsArticle.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc469cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6493"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length\n",
    "len(news_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f52f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24133682427229325"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lexical Diversity\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "lexical_diversity(news_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "287226cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', ',', 'if', 'you', 'use', 'the', 'example', 'of', 'planning', 'a', 'trip', ',', 'you', 'can', 'then', 'follow', 'up', 'with', 'additional', 'questions', 'like', '“', 'How', 'much', 'will', 'this', 'trip', 'cost', 'us', '?”', 'or', '“', 'Can', 'we', 'add', 'or', 'change', 'something', 'in', 'the', 'itinerary', '?”', '--', 'Ashley', 'Capoot', 'TUE', ',', 'FEB', '7', '20231', ':', '16', 'PM', 'EST', 'Nadella', 'promises', 'a', '‘', 'new', 'paradigm', 'for', 'search', '’', 'Nadella', 'discussed', 'some', 'of', 'the', 'work', 'the', 'company', 'is', 'doing', 'with', 'AI', ',', 'specifically', 'referring', 'to', 'search', '.']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "#Longest sentence\n",
    "news_sentences = myCorpus.sents('newsArticle.txt')\n",
    "\n",
    "\n",
    "\n",
    "longestSentN=[]\n",
    "for i in news_sentences:\n",
    "    if len(i)>len(longestSentN):\n",
    "        longestSentN=i\n",
    "\n",
    "print(longestSentN)\n",
    "\n",
    "#Number of elements in the sentence\n",
    "print(len(longestSentN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49debc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'if', 'you', 'use', 'the', 'example', 'of', 'planning', 'a', 'trip', 'you', 'can', 'then', 'follow', 'up', 'with', 'additional', 'questions', 'like', 'How', 'much', 'will', 'this', 'trip', 'cost', 'us', 'or', 'Can', 'we', 'add', 'or', 'change', 'something', 'in', 'the', 'itinerary', 'Ashley', 'Capoot', 'TUE', 'FEB', '7', '20231', '16', 'PM', 'EST', 'Nadella', 'promises', 'a', 'new', 'paradigm', 'for', 'search', 'Nadella', 'discussed', 'some', 'of', 'the', 'work', 'the', 'company', 'is', 'doing', 'with', 'AI', 'specifically', 'referring', 'to', 'search']\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "#Number of words in the sentence\n",
    "wordsSentN=[]\n",
    "\n",
    "for i in longestSentN:\n",
    "    if i.isalnum():\n",
    "        wordsSentN.append(i)\n",
    "        \n",
    "print(wordsSentN)\n",
    "\n",
    "print(len(wordsSentN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ad561d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data; language model; large corpus; wide range; provide\n",
      "information; Jordan Novet; responses may; Jonathan Vanian; typewriter\n",
      "effect; language models; server requirements; job market; best\n",
      "practices; answering questions; Teams Premium; phishing attacks;\n",
      "relied upon; try beginning; users simultaneously; chat conversations\n"
     ]
    }
   ],
   "source": [
    "#Top Collocations\n",
    "news = nltk.Text(myCorpus.words('newsArticle.txt'))\n",
    "news.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4ec97f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 201),\n",
       " ('a', 120),\n",
       " ('are', 43),\n",
       " ('as', 40),\n",
       " ('an', 22),\n",
       " ('also', 20),\n",
       " ('about', 19),\n",
       " ('all', 13),\n",
       " ('ability', 9),\n",
       " ('at', 9)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten words that start with each of the vowels\n",
    "alistN=sorted((w.lower() for w in news_article if w.startswith('a')))\n",
    "\n",
    "vowelDist=FreqDist(alistN)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0af5b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('even', 8),\n",
       " ('example', 8),\n",
       " ('enables', 7),\n",
       " ('employees', 5),\n",
       " ('engine', 5),\n",
       " ('emotions', 4),\n",
       " ('engaging', 4),\n",
       " ('engineer', 4),\n",
       " ('effect', 3),\n",
       " ('efforts', 3)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist2N=sorted((w.lower() for w in news_article if w.startswith('e')))\n",
    "\n",
    "vowelDist=FreqDist(alist2N)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5090da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 79),\n",
       " ('is', 54),\n",
       " ('it', 46),\n",
       " ('information', 20),\n",
       " ('its', 20),\n",
       " ('important', 8),\n",
       " ('into', 8),\n",
       " ('impact', 7),\n",
       " ('interactions', 7),\n",
       " ('improve', 6)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist3N=sorted((w.lower() for w in news_article if w.startswith('i')))\n",
    "\n",
    "vowelDist=FreqDist(alist3N)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8465152f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 151),\n",
       " ('on', 73),\n",
       " ('or', 40),\n",
       " ('one', 11),\n",
       " ('other', 7),\n",
       " ('order', 5),\n",
       " ('over', 5),\n",
       " ('only', 4),\n",
       " ('original', 4),\n",
       " ('often', 3)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist4N=sorted((w.lower() for w in news_article if w.startswith('o')))\n",
    "\n",
    "vowelDist=FreqDist(alist4N)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9636ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('users', 19),\n",
       " ('use', 12),\n",
       " ('understand', 10),\n",
       " ('used', 10),\n",
       " ('using', 7),\n",
       " ('usage', 6),\n",
       " ('user', 5),\n",
       " ('understanding', 4),\n",
       " ('up', 4),\n",
       " ('unique', 3)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist5N=sorted((w.lower() for w in news_article if w.startswith('u')))\n",
    "\n",
    "vowelDist=FreqDist(alist5N)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7568056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', ',', 'if', 'you', 'us', 'the', 'exampl', 'of', 'plan', 'a', 'trip', ',', 'you', 'can', 'then', 'follow', 'up', 'with', 'addit', 'quest', 'lik', '“', 'how', 'much', 'wil', 'thi', 'trip', 'cost', 'us', '?”', 'or', '“', 'can', 'we', 'ad', 'or', 'chang', 'someth', 'in', 'the', 'itin', '?”', '--', 'ashley', 'capoot', 'tue', ',', 'feb', '7', '20231', ':', '16', 'pm', 'est', 'nadell', 'prom', 'a', '‘', 'new', 'paradigm', 'for', 'search', '’', 'nadell', 'discuss', 'som', 'of', 'the', 'work', 'the', 'company', 'is', 'doing', 'with', 'ai', ',', 'spec', 'refer', 'to', 'search', '.']\n",
      "['so', ',', 'if', 'you', 'use', 'the', 'exampl', 'of', 'plan', 'a', 'trip', ',', 'you', 'can', 'then', 'follow', 'up', 'with', 'addit', 'question', 'like', '“', 'how', 'much', 'will', 'thi', 'trip', 'cost', 'us', '?”', 'or', '“', 'can', 'we', 'add', 'or', 'chang', 'someth', 'in', 'the', 'itinerari', '?”', '--', 'ashley', 'capoot', 'tue', ',', 'feb', '7', '20231', ':', '16', 'pm', 'est', 'nadella', 'promis', 'a', '‘', 'new', 'paradigm', 'for', 'search', '’', 'nadella', 'discuss', 'some', 'of', 'the', 'work', 'the', 'compani', 'is', 'do', 'with', 'ai', ',', 'specif', 'refer', 'to', 'search', '.']\n"
     ]
    }
   ],
   "source": [
    "#A stemmed version of the longest sentence \n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "Stem1=[lancaster.stem(t) for t in longestSentN]\n",
    "\n",
    "Stem2=[porter.stem(t) for t in longestSentN]\n",
    "\n",
    "print(Stem1)\n",
    "print(Stem2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9f7d9",
   "metadata": {},
   "source": [
    "## Subcorpus 3 - Recipes\n",
    "The file \"Recipes.txt\" is the third and last component of our corpus. This file is a compilation of different recipes. The recipes were retrieved from the Rachael Ray Show website (https://www.rachaelrayshow.com/recipes/four-dried-chilies-chili-with-cocoa-cinnamon-rachael-ray). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "badceaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_words = myCorpus.words('Recipes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "608bb9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7967"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length\n",
    "len(recipes_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad049901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16354964227438182"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lexical Diversity\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "lexical_diversity(recipes_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b2a09f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ingredients', 'For', 'the', 'Sauce', ':', '2', 'tablespoons', 'EVOO', '2', 'tablespoons', 'butter', '1', 'small', 'onion', ',', 'finely', 'chopped', 'Salt', '3', 'cloves', 'garlic', ',', 'chopped', 'or', 'grated', 'One', '14', '-', 'ounce', 'can', 'Italian', 'cherry', 'tomatoes', 'or', 'diced', 'tomatoes', '2', 'cups', 'passata', '(', 'tomato', 'puree', ')', '1', 'cup', 'beef', 'stock', 'or', 'beef', 'bone', 'broth', '1', 'teaspoon', 'sugar', 'or', 'honey', '1', 'bay', 'leaf', ',', 'optional', 'For', 'the', 'Cabbage', 'and', 'Meat', 'Filling', ':', '1', 'large', 'green', 'cabbage', 'Fine', 'sea', 'salt', '¾', 'cup', 'raw', 'white', 'rice', '½', 'cup', 'white', 'wine', '½', 'cup', 'beef', 'stock', '1', '½', 'pounds', 'ground', 'beef', ',', '80', '%', '½', 'pound', 'ground', 'pork', 'Salt', 'and', 'pepper', '2', 'teaspoons', 'each', 'granulated', 'onion', 'and', 'granulated', 'garlic', '2', 'teaspoons', 'fennel', 'seed', 'or', 'fennel', 'pollen', 'or', 'a', 'combo', '2', 'tablespoons', 'fresh', 'sage', ',', 'finely', 'chopped', ',', 'or', '2', 'teaspoons', 'dried', 'sage', '1', 'ounce', 'each', 'grated', 'pecorino', 'and', 'grated', 'Parm', '(', 'about', '¾', 'cup', 'each', ')', '2', 'tablespoons', 'EVOO', '1', 'egg', ',', 'lightly', 'beaten', 'Handful', 'of', 'parsley', ',', 'chopped', 'For', 'the', 'Cabbage', 'and', 'Meat', 'Filling', ':', '1', 'teaspoon', 'red', 'pepper', 'flakes', 'or', 'ground', 'pepperoncini', 'Ingredients', '2', 'pounds', 'diced', 'orange', 'squash', '(', 'kabocha', ',', 'pumpkin', 'or', 'butternut', ')', '4', 'cloves', 'garlic', ',', 'crushed', 'EVOO', 'non', '-', 'aerosol', 'spray', 'Salt', 'and', 'pepper', 'or', 'red', 'pepper', 'flakes', 'Freshly', 'grated', 'nutmeg', ',', 'about', '¼', 'teaspoon', '1', 'teaspoon', 'each', 'lemon', 'and', 'orange', 'zest', '1', '½', 'cups', 'chicken', 'or', 'vegetable', 'broth', 'or', 'stock', '½', 'lemon', 'and', '¼', 'orange', ',', 'aka', 'an', 'orange', 'cheek', '¾', 'cup', 'walnuts', 'or', 'hazelnuts', '5', 'tablespoons', 'butter', '24', 'sage', 'leaves', '1', 'pound', 'gemelli', 'or', 'strozzapreti', 'or', 'penne', 'or', 'pumpkin', 'pasta', 'in', 'short', 'cut', '1', 'cup', 'grated', 'Parmigiano', '-', 'Reggiano', 'cheese', ',', 'plus', 'some', 'to', 'pass', '½', 'cup', 'toasted', 'pumpkin', 'seeds', ',', 'optional', 'Yield', 'Serves', ':', '4', 'to', '6', 'Preparation', 'Heat', 'a', 'large', 'pot', 'of', 'water', 'to', 'boil', 'for', 'pasta', '.']\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#Longest sentence\n",
    "recipes_sentences = myCorpus.sents('Recipes.txt')\n",
    "\n",
    "\n",
    "\n",
    "longestSentR=[]\n",
    "for i in recipes_sentences:\n",
    "    if len(i)>len(longestSentR):\n",
    "        longestSentR=i\n",
    "\n",
    "print(longestSentR)\n",
    "\n",
    "#Number of elements in the sentence\n",
    "print(len(longestSentR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc348ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ingredients', 'For', 'the', 'Sauce', '2', 'tablespoons', 'EVOO', '2', 'tablespoons', 'butter', '1', 'small', 'onion', 'finely', 'chopped', 'Salt', '3', 'cloves', 'garlic', 'chopped', 'or', 'grated', 'One', '14', 'ounce', 'can', 'Italian', 'cherry', 'tomatoes', 'or', 'diced', 'tomatoes', '2', 'cups', 'passata', 'tomato', 'puree', '1', 'cup', 'beef', 'stock', 'or', 'beef', 'bone', 'broth', '1', 'teaspoon', 'sugar', 'or', 'honey', '1', 'bay', 'leaf', 'optional', 'For', 'the', 'Cabbage', 'and', 'Meat', 'Filling', '1', 'large', 'green', 'cabbage', 'Fine', 'sea', 'salt', '¾', 'cup', 'raw', 'white', 'rice', '½', 'cup', 'white', 'wine', '½', 'cup', 'beef', 'stock', '1', '½', 'pounds', 'ground', 'beef', '80', '½', 'pound', 'ground', 'pork', 'Salt', 'and', 'pepper', '2', 'teaspoons', 'each', 'granulated', 'onion', 'and', 'granulated', 'garlic', '2', 'teaspoons', 'fennel', 'seed', 'or', 'fennel', 'pollen', 'or', 'a', 'combo', '2', 'tablespoons', 'fresh', 'sage', 'finely', 'chopped', 'or', '2', 'teaspoons', 'dried', 'sage', '1', 'ounce', 'each', 'grated', 'pecorino', 'and', 'grated', 'Parm', 'about', '¾', 'cup', 'each', '2', 'tablespoons', 'EVOO', '1', 'egg', 'lightly', 'beaten', 'Handful', 'of', 'parsley', 'chopped', 'For', 'the', 'Cabbage', 'and', 'Meat', 'Filling', '1', 'teaspoon', 'red', 'pepper', 'flakes', 'or', 'ground', 'pepperoncini', 'Ingredients', '2', 'pounds', 'diced', 'orange', 'squash', 'kabocha', 'pumpkin', 'or', 'butternut', '4', 'cloves', 'garlic', 'crushed', 'EVOO', 'non', 'aerosol', 'spray', 'Salt', 'and', 'pepper', 'or', 'red', 'pepper', 'flakes', 'Freshly', 'grated', 'nutmeg', 'about', '¼', 'teaspoon', '1', 'teaspoon', 'each', 'lemon', 'and', 'orange', 'zest', '1', '½', 'cups', 'chicken', 'or', 'vegetable', 'broth', 'or', 'stock', '½', 'lemon', 'and', '¼', 'orange', 'aka', 'an', 'orange', 'cheek', '¾', 'cup', 'walnuts', 'or', 'hazelnuts', '5', 'tablespoons', 'butter', '24', 'sage', 'leaves', '1', 'pound', 'gemelli', 'or', 'strozzapreti', 'or', 'penne', 'or', 'pumpkin', 'pasta', 'in', 'short', 'cut', '1', 'cup', 'grated', 'Parmigiano', 'Reggiano', 'cheese', 'plus', 'some', 'to', 'pass', '½', 'cup', 'toasted', 'pumpkin', 'seeds', 'optional', 'Yield', 'Serves', '4', 'to', '6', 'Preparation', 'Heat', 'a', 'large', 'pot', 'of', 'water', 'to', 'boil', 'for', 'pasta']\n",
      "271\n"
     ]
    }
   ],
   "source": [
    "#Number of words in the sentence\n",
    "wordsSentR=[]\n",
    "\n",
    "for i in longestSentR:\n",
    "    if i.isalnum():\n",
    "        wordsSentR.append(i)\n",
    "        \n",
    "print(wordsSentR)\n",
    "\n",
    "print(len(wordsSentR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce041764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finely chopped; cloves garlic; Yield Serves; olive oil; stirring\n",
      "often; mousse pate; Preheat oven; thinly sliced; egg wash; iron\n",
      "skillet; baking sheet; bay leaf; ground beef; pounds ground; sage\n",
      "leaves; says Rach; Dutch oven; Individual Beef; puff pastry; black\n",
      "pepper\n"
     ]
    }
   ],
   "source": [
    "#Top Collocations\n",
    "recipe = nltk.Text(myCorpus.words('Recipes.txt'))\n",
    "recipe.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e03af4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 313),\n",
       " ('a', 117),\n",
       " ('about', 51),\n",
       " ('add', 29),\n",
       " ('as', 18),\n",
       " ('are', 11),\n",
       " ('at', 11),\n",
       " ('all', 6),\n",
       " ('aerosol', 5),\n",
       " ('absorb', 4)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten words that start with each of the vowels\n",
    "alistR=sorted((w.lower() for w in recipes_words if w.startswith('a')))\n",
    "\n",
    "vowelDist=FreqDist(alistR)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34bfe8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('each', 24),\n",
       " ('egg', 13),\n",
       " ('eggs', 10),\n",
       " ('even', 6),\n",
       " ('easy', 5),\n",
       " ('eggplant', 5),\n",
       " ('end', 3),\n",
       " ('extra', 3),\n",
       " ('enough', 2),\n",
       " ('escarole', 2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist2R=sorted((w.lower() for w in recipes_words if w.startswith('e')))\n",
    "\n",
    "vowelDist=FreqDist(alist2R)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97d0871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 67),\n",
       " ('it', 36),\n",
       " ('into', 20),\n",
       " ('inch', 17),\n",
       " ('is', 14),\n",
       " ('inches', 8),\n",
       " ('iron', 6),\n",
       " ('if', 4),\n",
       " ('ingredients', 3),\n",
       " ('includes', 1)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist3R=sorted((w.lower() for w in recipes_words if w.startswith('i')))\n",
    "\n",
    "vowelDist=FreqDist(alist3R)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "807b678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('or', 132),\n",
       " ('of', 82),\n",
       " ('oil', 33),\n",
       " ('on', 32),\n",
       " ('over', 27),\n",
       " ('onion', 20),\n",
       " ('oven', 20),\n",
       " ('onions', 17),\n",
       " ('olive', 14),\n",
       " ('ounces', 10)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist4R=sorted((w.lower() for w in recipes_words if w.startswith('o')))\n",
    "\n",
    "vowelDist=FreqDist(alist4R)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40ce759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('until', 40),\n",
       " ('up', 9),\n",
       " ('using', 5),\n",
       " ('use', 4),\n",
       " ('used', 3),\n",
       " ('unsweetened', 1),\n",
       " ('upstate', 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist5R=sorted((w.lower() for w in recipes_words if w.startswith('u')))\n",
    "\n",
    "vowelDist=FreqDist(alist5R)\n",
    "vowelDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d719a69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ingredy', 'for', 'the', 'sauc', ':', '2', 'tablespoon', 'evoo', '2', 'tablespoon', 'but', '1', 'smal', 'on', ',', 'fin', 'chop', 'salt', '3', 'clov', 'garl', ',', 'chop', 'or', 'grat', 'on', '14', '-', 'ount', 'can', 'it', 'cherry', 'tomato', 'or', 'dic', 'tomato', '2', 'cup', 'passat', '(', 'tomato', 'pur', ')', '1', 'cup', 'beef', 'stock', 'or', 'beef', 'bon', 'bro', '1', 'teaspoon', 'sug', 'or', 'honey', '1', 'bay', 'leaf', ',', 'opt', 'for', 'the', 'cab', 'and', 'meat', 'fil', ':', '1', 'larg', 'green', 'cab', 'fin', 'sea', 'salt', '¾', 'cup', 'raw', 'whit', 'ric', '½', 'cup', 'whit', 'win', '½', 'cup', 'beef', 'stock', '1', '½', 'pound', 'ground', 'beef', ',', '80', '%', '½', 'pound', 'ground', 'pork', 'salt', 'and', 'pep', '2', 'teaspoon', 'each', 'gran', 'on', 'and', 'gran', 'garl', '2', 'teaspoon', 'fennel', 'see', 'or', 'fennel', 'pol', 'or', 'a', 'combo', '2', 'tablespoon', 'fresh', 'sag', ',', 'fin', 'chop', ',', 'or', '2', 'teaspoon', 'dri', 'sag', '1', 'ount', 'each', 'grat', 'pecorino', 'and', 'grat', 'parm', '(', 'about', '¾', 'cup', 'each', ')', '2', 'tablespoon', 'evoo', '1', 'eg', ',', 'light', 'beat', 'hand', 'of', 'parsley', ',', 'chop', 'for', 'the', 'cab', 'and', 'meat', 'fil', ':', '1', 'teaspoon', 'red', 'pep', 'flak', 'or', 'ground', 'pepperoncin', 'ingredy', '2', 'pound', 'dic', 'orang', 'squash', '(', 'kaboch', ',', 'pumpkin', 'or', 'butternut', ')', '4', 'clov', 'garl', ',', 'crush', 'evoo', 'non', '-', 'aerosol', 'spray', 'salt', 'and', 'pep', 'or', 'red', 'pep', 'flak', 'fresh', 'grat', 'nutmeg', ',', 'about', '¼', 'teaspoon', '1', 'teaspoon', 'each', 'lemon', 'and', 'orang', 'zest', '1', '½', 'cup', 'chick', 'or', 'veget', 'bro', 'or', 'stock', '½', 'lemon', 'and', '¼', 'orang', ',', 'ak', 'an', 'orang', 'cheek', '¾', 'cup', 'walnut', 'or', 'hazelnut', '5', 'tablespoon', 'but', '24', 'sag', 'leav', '1', 'pound', 'gemell', 'or', 'strozzapreti', 'or', 'pen', 'or', 'pumpkin', 'past', 'in', 'short', 'cut', '1', 'cup', 'grat', 'parmigiano', '-', 'reggiano', 'chees', ',', 'plu', 'som', 'to', 'pass', '½', 'cup', 'toast', 'pumpkin', 'see', ',', 'opt', 'yield', 'serv', ':', '4', 'to', '6', 'prep', 'heat', 'a', 'larg', 'pot', 'of', 'wat', 'to', 'boil', 'for', 'past', '.']\n",
      "['ingredi', 'for', 'the', 'sauc', ':', '2', 'tablespoon', 'evoo', '2', 'tablespoon', 'butter', '1', 'small', 'onion', ',', 'fine', 'chop', 'salt', '3', 'clove', 'garlic', ',', 'chop', 'or', 'grate', 'one', '14', '-', 'ounc', 'can', 'italian', 'cherri', 'tomato', 'or', 'dice', 'tomato', '2', 'cup', 'passata', '(', 'tomato', 'pure', ')', '1', 'cup', 'beef', 'stock', 'or', 'beef', 'bone', 'broth', '1', 'teaspoon', 'sugar', 'or', 'honey', '1', 'bay', 'leaf', ',', 'option', 'for', 'the', 'cabbag', 'and', 'meat', 'fill', ':', '1', 'larg', 'green', 'cabbag', 'fine', 'sea', 'salt', '¾', 'cup', 'raw', 'white', 'rice', '½', 'cup', 'white', 'wine', '½', 'cup', 'beef', 'stock', '1', '½', 'pound', 'ground', 'beef', ',', '80', '%', '½', 'pound', 'ground', 'pork', 'salt', 'and', 'pepper', '2', 'teaspoon', 'each', 'granul', 'onion', 'and', 'granul', 'garlic', '2', 'teaspoon', 'fennel', 'seed', 'or', 'fennel', 'pollen', 'or', 'a', 'combo', '2', 'tablespoon', 'fresh', 'sage', ',', 'fine', 'chop', ',', 'or', '2', 'teaspoon', 'dri', 'sage', '1', 'ounc', 'each', 'grate', 'pecorino', 'and', 'grate', 'parm', '(', 'about', '¾', 'cup', 'each', ')', '2', 'tablespoon', 'evoo', '1', 'egg', ',', 'lightli', 'beaten', 'hand', 'of', 'parsley', ',', 'chop', 'for', 'the', 'cabbag', 'and', 'meat', 'fill', ':', '1', 'teaspoon', 'red', 'pepper', 'flake', 'or', 'ground', 'pepperoncini', 'ingredi', '2', 'pound', 'dice', 'orang', 'squash', '(', 'kabocha', ',', 'pumpkin', 'or', 'butternut', ')', '4', 'clove', 'garlic', ',', 'crush', 'evoo', 'non', '-', 'aerosol', 'spray', 'salt', 'and', 'pepper', 'or', 'red', 'pepper', 'flake', 'freshli', 'grate', 'nutmeg', ',', 'about', '¼', 'teaspoon', '1', 'teaspoon', 'each', 'lemon', 'and', 'orang', 'zest', '1', '½', 'cup', 'chicken', 'or', 'veget', 'broth', 'or', 'stock', '½', 'lemon', 'and', '¼', 'orang', ',', 'aka', 'an', 'orang', 'cheek', '¾', 'cup', 'walnut', 'or', 'hazelnut', '5', 'tablespoon', 'butter', '24', 'sage', 'leav', '1', 'pound', 'gemelli', 'or', 'strozzapreti', 'or', 'penn', 'or', 'pumpkin', 'pasta', 'in', 'short', 'cut', '1', 'cup', 'grate', 'parmigiano', '-', 'reggiano', 'chees', ',', 'plu', 'some', 'to', 'pass', '½', 'cup', 'toast', 'pumpkin', 'seed', ',', 'option', 'yield', 'serv', ':', '4', 'to', '6', 'prepar', 'heat', 'a', 'larg', 'pot', 'of', 'water', 'to', 'boil', 'for', 'pasta', '.']\n"
     ]
    }
   ],
   "source": [
    "#A stemmed version of the longest sentence \n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "Stem1=[lancaster.stem(t) for t in longestSentR]\n",
    "\n",
    "Stem2=[porter.stem(t) for t in longestSentR]\n",
    "\n",
    "print(Stem1)\n",
    "print(Stem2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
